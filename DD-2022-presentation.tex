\documentclass[10pt]{beamer}

\usepackage{polyglossia}
\usepackage{csquotes}
\usepackage{bm}
\usepackage{datetime}
\usepackage{fontspec}
\usepackage{microtype}
\usepackage{color}
\usepackage{url}
\usepackage{pgfplots}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{subcaption}
\usepackage[backend=biber,style=iso-authoryear,sortlocale=en_US,autolang=other,bibencoding=UTF8]{biblatex}
\usepackage{booktabs}
\usepackage{graphics}
\usepackage{pifont}
\usepackage{ulem}
\usepackage{tikz}

\usepgfplotslibrary{fillbetween}
\usetikzlibrary{patterns}

\addbibresource{zotero.bib}

\setdefaultlanguage{english}
\setmainfont{TeX Gyre Termes}
\usetheme{Boadilla}
\usecolortheme{crane}
\setbeamertemplate{title page}[default][rounded=true,shadow=false]
\setbeamertemplate{section in toc}[ball unnumbered]
\setbeamertemplate{bibliography item}{}

\hypersetup{
	pdfencoding=auto,
	unicode=true,
	citecolor=green,
	filecolor=blue,
	linkcolor=red,
	urlcolor=blue
}

\makeatletter
\newcommand*{\currentSection}{\@currentlabelname}
\makeatother

\newcommand{\mathmat}{\ensuremath{\mathbf}}

\title[DDny 2022]
{
	Scalable Graph Size Reduction for Efficient GNN Application
}

\newdate{presentation}{11}{11}{2022}
\date[November 2022]{Doktorandské dny KM FJFI, \displaydate{presentation}}

\author[Procházka et al.]{
	Pavel Procházka\inst{1} \and
	Michal Mareš\inst{1,2} \and
	\underline{Marek Dědič}\inst{1,2}
}
\institute[Cisco \& CTU]{
	\inst{1}Cisco Systems, Inc. \and
	\inst{2}Czech Technical University in Prague
}

% Title card
\AtBeginSection[]{
	\begin{frame}
	\vfill
	\centering
	\begin{beamercolorbox}[sep = 8pt,center,shadow = true,rounded = true]{title}
		\usebeamerfont{title}\insertsectionhead\par%
	\end{beamercolorbox}
	\vfill
\end{frame}
}

\begin{document}

\begin{frame}
	\titlepage
\end{frame}

\begin{frame}{Motivation}
	\begin{itemize}
	    \item In Cisco, we perform node classification on large graphs to identify malicious domains
	    \begin{itemize}
	        \item Currently simple, scalable models
	    \end{itemize}
	    \item State of the art: GNNs
	    \begin{itemize}
	        \item Powerful but computationally demanding
	        \item Do not scale to our data (at reasonable cost)
	    \end{itemize}
	    \item Suggested solution: compressing the input graph
    \end{itemize}
\end{frame}

\begin{frame}{Main idea}
    \input{images/coarsening-illustration}
\end{frame}

\begin{frame}{Details}
    \begin{itemize}
        \item Base model $\to$ posterior for all nodes
        \item Similarity measure
        \item Edge contraction $\to$ graph sequence
        \item GNN on each graph with label propagation
    \end{itemize}
    \vspace{1cm}
    \scalebox{0.75}{\input{images/graph-sequence}}
\end{frame}

\begin{frame}{Complexity performance trade-off}
	\input{images/complexity-performance}
\end{frame}

\begin{frame}{Complexity}
	\begin{itemize}
	    \item usually time \& memory complexity
        \item in business we can translate time to cost
        \begin{itemize}
            \item more interpretable
            \item monthly budget \$100, EKS cost \$0.33/hour
            \item therefore daily run can take 10 hours $to$ select graph size accordingly
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Edge contraction induced hierarchical tree}
	\centerline{\scalebox{0.75}{\input{images/graph-seq-uncover}}}
	\centerline{\scalebox{0.75}{\input{images/tree-clustering}}}
\end{frame}

\begin{frame}
	\frametitle{Shared prediction induced performance upper bound}
    \centering{\scalebox{1.2}{\input{images/upper-bound}}}
\end{frame}

\begin{frame}{Experiment Setup}
    \begin{itemize}
        \item Base model: logistic regression
        \item Similarity measure: KL divergence
        \item GNN: 2-layer GCN
        \item Dataset: Cora
    \end{itemize}
\end{frame}

\begin{frame}{Comparison of Performance Given by Probability Similarities}
    \input{images/cora-similarities}
\end{frame}

\begin{frame}{Conclusions \& Future Work}
    \begin{itemize}
        \item Conclusions
        \begin{itemize}
            \item Performance-complexity trade-off
            \begin{itemize}
                \item Ability to set cost limit for running the algorithm
            \end{itemize}
            \item Hierarchical tree as a byproduct
            \item Computationally cheap upper bound
            \item Transfer learning: like using other hyperparameters
            \begin{itemize}
                \item Assuming the data does not change significantly
            \end{itemize}
        \end{itemize}
        
        \item Future Work
        \begin{itemize}
            \item Run on our datasets
            \item Use for smarter coarsening of riskmap input graph
            \begin{itemize}
                \item Current coarsening rather simple: drop nodes with deg < 3  
            \end{itemize}
        \end{itemize}
        
    \item More details can be found in the ITAT paper \cite{prochazka_scalable_2022}
    \item My related work presented at ECML \cite{dedic_adaptive_2022}
    \end{itemize}
\end{frame}

\begin{frame}
	\titlepage
\end{frame}

\end{document}
