
@inproceedings{prochazka_scalable_2022,
	address = {Zuberec, Slovakia},
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Scalable {Graph} {Size} {Reduction} for {Efficient} {GNN} {Application}},
	volume = {3226},
	copyright = {All rights reserved},
	url = {http://ceur-ws.org/Vol-3226/#paper9},
	abstract = {Graph neural networks (GNN) present a dominant framework for representation learning on graphs for the past several years. The main strength of GNNs lies in the fact that they can simultaneously learn from both node related attributes and relations between nodes, represented by edges. In tasks leading to large graphs, GNN often requires significant computational resources to achieve its superior performance. In order to reduce the computational cost, methods allowing for a flexible balance between complexity and performance could be useful. In this work, we propose a simple scalable task-aware graph preprocessing procedure allowing us to obtain a reduced graph such as GNN achieves a given desired performance on the downstream task. In addition, the proposed preprocessing allows for fitting the reduced graph and GNN into a given memory/computational resources. The proposed preprocessing is evaluated and compared with several reference scenarios on conventional GNN benchmark datasets.},
	language = {en},
	urldate = {2022-09-30},
	booktitle = {Proceedings of the 22nd {Conference} {Information} {Technologies} – {Applications} and {Theory} ({ITAT} 2022)},
	publisher = {CEUR-WS.org},
	author = {Procházka, Pavel and Mareš, Michal and Dědič, Marek},
	month = sep,
	year = {2022},
	pages = {75--84},
}

@inproceedings{dedic_adaptive_2022,
	address = {Grenoble, France},
	title = {Adaptive graph coarsening in the context of local graph quality},
	copyright = {All rights reserved},
	url = {https://graphquality.github.io/rsc/art1.pdf},
	abstract = {Graph based models are used for tasks with increasing size and computational demands. We present a method for studying graph properties from the point of view of a downstream task. More precisely, the method allows a user to precisely select the resolution at which the graph in question should be coarsened. Our method builds on an existing algorithm for pretraining on coarser graphs, HARP. We extend both main parts of the algorithm in order to observe the effect of graph coarsening to model quality on a fine level. We present a general framework for graph coarsenings, providing two alternative algorithms based on graph diffusion convolution and evolutionary algorithms. Additionally, we present a novel way for un-coarsening the reduced graph in a targeted way based on the confidence of downstream classification for particular nodes. Together, these enhancements provide sufficient detail where needed, while collapsing structures where per-node information is not necessary for high model performance. Our method is a general meta-model for enhancing graph embedding models such as node2vec. We apply the method to several datasets and discuss the differing behaviour on each of them. Furthermore, we compare the proposed coarsening schemas.},
	language = {en},
	urldate = {2022-09-23},
	author = {Dědič, Marek and Bajer, Lukáš and Repický, Jakub and Procházka, Pavel and Holeňa, Martin},
	month = sep,
	year = {2022},
}
